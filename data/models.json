{
  "mmlu": { "labels": ["Avg Human", "Llama 3", "GPT-4", "Claude 3"], "scores": [34.5, 79.5, 86.4, 88.2] },
  "hellaswag": { "labels": ["Avg Human", "GPT-3.5", "GPT-4", "Claude 3"], "scores": [73.0, 85.5, 95.3, 96.1] },
  "arc": { "labels": ["Avg Human", "Llama 2", "GPT-4", "Claude 3"], "scores": [80.0, 65.0, 96.3, 95.8] },
  "mathqa": { "labels": ["Avg Human", "GPT-3.5", "GPT-4", "Google Gemini"], "scores": [60.0, 72.0, 91.0, 93.0] },
  "truthfulqa": { "labels": ["GPT-3.5", "GPT-4", "Claude 3", "Human Expert"], "scores": [48.0, 60.0, 75.0, 94.0] },
  "race": { "labels": ["Avg Human", "GPT-3.5", "GPT-4", "Llama 3"], "scores": [54.0, 81.0, 89.0, 91.0] },
  
  "aqua": { "labels": ["Avg Human", "PaLM 2", "GPT-4", "Claude 3"], "scores": [45.0, 68.0, 82.0, 84.5] },
  "winogrande": { "labels": ["Avg Human", "GPT-3.5", "GPT-4", "Gemini 1.5"], "scores": [94.0, 78.0, 92.5, 93.0] },
  "commonsense": { "labels": ["Avg Human", "Llama 2", "GPT-4", "Claude 3"], "scores": [89.0, 78.0, 94.0, 95.5] },
  "sciq": { "labels": ["Avg Human", "GPT-3.5", "GPT-4", "Llama 3"], "scores": [85.0, 93.0, 98.0, 98.5] },
  "piqa": { "labels": ["Avg Human", "GPT-3.5", "GPT-4", "Claude 3"], "scores": [80.0, 84.0, 92.0, 94.0] },
  "openbook": { "labels": ["Avg Human", "Llama 2", "GPT-4", "Gemini Ultra"], "scores": [82.0, 86.0, 95.9, 96.5] }
}